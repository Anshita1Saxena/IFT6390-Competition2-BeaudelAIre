{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "import re\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import training and test corpus\n",
    "df_train = pd.read_csv('../kaggle-competition-2/train_data.csv')\n",
    "df_test = pd.read_csv('../kaggle-competition-2/test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    sentences = df.copy()\n",
    "    # Converting all the upper case to lower case to avoid the distinction between them\n",
    "    sentences['text'] = df['text'].str.lower()\n",
    "    # Putting the regex for removing the https and www URLs\n",
    "    sentences['text'] = sentences['text'].apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
    "    sentences['text'] = sentences['text'].apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
    "\n",
    "    # Remove the video and links\n",
    "    sentences['text'] = sentences['text'].apply(lambda x: re.sub(r'{link}', '', x))\n",
    "    sentences['text'] = sentences['text'].apply(lambda x: re.sub(r\"\\[video\\]\", '', x))\n",
    "\n",
    "    # Remove html reference characters\n",
    "    sentences['text'] = sentences['text'].apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n",
    "\n",
    "    # Remove usernames\n",
    "    sentences['text'] = sentences['text'].apply(lambda x: re.sub(r'@[^\\s]+', '', x))\n",
    "\n",
    "    # Removing numbers\n",
    "    sentences['text'] = sentences['text'].apply(lambda x: re.sub(r'\\d+', '', x))\n",
    "\n",
    "    # Removing hashmarks, non-letter characters\n",
    "    sentences['text'] = sentences['text'].apply(lambda x: re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\/\\];='#]\", '', x))\n",
    "        \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess the senteces \n",
    "train_proc = preprocessing(df_train)\n",
    "test_proc = preprocessing(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the encoder corpus\n",
    "enc_corp = pd.concat([train_proc['text'],test_proc['text']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec : CBOW and SkipGram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           [anyway, im, getting, of, for, a, while, ]\n",
       "1    [my, red, apache, isn't, feelin, too, well, th...\n",
       "2    [, you, should, be, , its, great, friday, will...\n",
       "3    [its, :pm, and, i, dont, wanna, sleep;, so, i,...\n",
       "4    [why, does, twitter, eat, my, dm's, , not, hap...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Transform sentences to list of words\n",
    "sentences_corp = enc_corp.apply(lambda x: x.split(' '))\n",
    "sentences_corp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Word2Vec encoding with Skip-Gram and vector size of 100\n",
    "word2vec_SkipGram_100d = Word2Vec(sentences=sentences_corp, sg=1, vector_size=100, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Word2Vec encoding with Skip-Gram and vector size of 300\n",
    "word2vec_SkipGram_300d = Word2Vec(sentences=sentences_corp, sg=1, vector_size=300, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Word2Vec encoding with Skip-Gram and vector size of 100\n",
    "word2vec_CBOW_100d = Word2Vec(sentences=sentences_corp, sg=0, vector_size=100, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train Word2Vec encoding with Skip-Gram and vector size of 300\n",
    "word2vec_CBOW_300d = Word2Vec(sentences=sentences_corp, sg=0, vector_size=300, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59347\n",
      "59347\n",
      "59347\n",
      "59347\n"
     ]
    }
   ],
   "source": [
    "#Test our encoding\n",
    "print(len(list(word2vec_SkipGram_100d.wv.index_to_key)))\n",
    "print(len(list(word2vec_CBOW_100d.wv.index_to_key)))\n",
    "print(len(list(word2vec_SkipGram_300d.wv.index_to_key)))\n",
    "print(len(list(word2vec_CBOW_300d.wv.index_to_key)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('laptop', 0.9002227783203125), ('pc', 0.8388065099716187), ('comp', 0.8185139298439026), ('hiptop', 0.7889953851699829), ('lappy', 0.7790974378585815), ('compy', 0.7786740660667419), ('crackberry', 0.7741041779518127), ('lappie', 0.7612731456756592), ('internet', 0.7598783373832703), (\"phone's\", 0.7594239115715027)]\n",
      "[('laptop', 0.7479316592216492), ('comp', 0.7214232683181763), ('pc', 0.7158175110816956), ('compy', 0.6317026019096375), ('hiptop', 0.6210114359855652), ('lappy', 0.6167007684707642), ('puter', 0.612076461315155), ('lappie', 0.602472722530365), (\"computer's\", 0.6002691984176636), ('harddrive', 0.5867507457733154)]\n"
     ]
    }
   ],
   "source": [
    "#Find words similar to computer\n",
    "#We can see that the 300d vector seems less accurate\n",
    "print(word2vec_SkipGram_100d.wv.most_similar('computer'))\n",
    "print(word2vec_SkipGram_300d.wv.most_similar('computer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('laptop', 0.9102173447608948), ('pc', 0.8555576205253601), ('comp', 0.8389325141906738), ('phone', 0.7491770386695862), ('lappy', 0.743495762348175), ('crackberry', 0.7390615344047546), ('internet', 0.7363808751106262), ('router', 0.7212945818901062), ('keyboard', 0.7173010110855103), ('blackberry', 0.7102299332618713)]\n",
      "[('laptop', 0.8476789593696594), ('comp', 0.8177416324615479), ('pc', 0.8091326951980591), ('lappy', 0.692457914352417), ('crackberry', 0.6732901930809021), ('keyboard', 0.6669068932533264), ('internet', 0.6639253497123718), ('router', 0.6499288082122803), ('ipod', 0.6441949605941772), ('mbp', 0.6388487219810486)]\n"
     ]
    }
   ],
   "source": [
    "#Find words similar to computer\n",
    "#Once again the 300d vector seems less accurate\n",
    "print(word2vec_CBOW_100d.wv.most_similar('computer'))\n",
    "print(word2vec_CBOW_300d.wv.most_similar('computer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the models\n",
    "word2vec_SkipGram_100d.save('../Encoders/word2vec_SkipGram_100d')\n",
    "word2vec_CBOW_100d.save('../Encoders/word2vec_CBOW_100d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('laptop', 0.9102173447608948),\n",
       " ('pc', 0.8555576205253601),\n",
       " ('comp', 0.8389325141906738),\n",
       " ('phone', 0.7491770386695862),\n",
       " ('lappy', 0.743495762348175),\n",
       " ('crackberry', 0.7390615344047546),\n",
       " ('internet', 0.7363808751106262),\n",
       " ('router', 0.7212945818901062),\n",
       " ('keyboard', 0.7173010110855103),\n",
       " ('blackberry', 0.7102299332618713)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Example of loading model\n",
    "new_model = Word2Vec.load('../Encoders/word2vec_CBOW_100d')\n",
    "new_model.wv.most_similar('computer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "05d8f42d34e97b63e1918e352e5e1ee86173089fb3b8c3e567ea1c06d83cd6aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
