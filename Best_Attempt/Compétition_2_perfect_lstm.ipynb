{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cpd6o6snv4f2"
      },
      "source": [
        "#Compétition 2 - Défi de classification de texte  (Text Classification Challenge)\n",
        "Anshita Saxena, Denis Lemarchand, Vincent Gariépy <br/>\n",
        "Team: BaudelAIre <br/>\n",
        "Organization: University of Montreal (MILA) <br/>\n",
        "Course: IFT 6390"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4Pkv3flyNNg"
      },
      "source": [
        "#Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FoLvWqWEvy82"
      },
      "outputs": [],
      "source": [
        "# Basic Libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Word processing library\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "snowBallStemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Tokenizers and pad_sequences libraries\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Model Dependency libraries\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow import keras\n",
        "\n",
        "# Google drive library\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JcyTy6CXyQ6Y"
      },
      "source": [
        "##Upload Kaggle File from Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-uRO-bywXfn",
        "outputId": "b4b3c152-e1ca-4fe2-993f-fd8c0f0c43ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: PyDrive in /usr/local/lib/python3.7/dist-packages (1.3.1)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (6.0)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.7/dist-packages (from PyDrive) (1.12.11)\n",
            "Requirement already satisfied: google-api-core<3dev,>=1.21.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (2.8.2)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.17.4)\n",
            "Requirement already satisfied: six<2dev,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (1.15.0)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (3.0.1)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (0.0.4)\n",
            "Requirement already satisfied: google-auth<3dev,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from google-api-python-client>=1.2->PyDrive) (2.14.1)\n",
            "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.23.0)\n",
            "Requirement already satisfied: protobuf<5.0.0dev,>=3.15.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.19.6)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.57.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.2->PyDrive) (4.9)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.2->PyDrive) (5.2.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3dev,>=1.16.0->google-api-python-client>=1.2->PyDrive) (0.2.8)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from oauth2client>=4.0.0->PyDrive) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0dev,>=2.18.0->google-api-core<3dev,>=1.21.0->google-api-python-client>=1.2->PyDrive) (2.10)\n"
          ]
        }
      ],
      "source": [
        "#@title Install PyDrive\n",
        "!pip install PyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wNF-CChvyt9K"
      },
      "outputs": [],
      "source": [
        "# This notebook should access to Google Drive\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "nwyJrWUby2G3"
      },
      "outputs": [],
      "source": [
        "# train data\n",
        "# https://drive.google.com/file/d/17cD6hFS_AfKxtfjKxfeNUkC44jfSYkGj/view?usp=sharing\n",
        "# train result\n",
        "# https://drive.google.com/file/d/1iEpwQ3B4d4gIUl3ynq_p74U1QKZTURem/view?usp=sharing\n",
        "# test data\n",
        "# https://drive.google.com/file/d/1QbhqakgLpBXWr5sMFSe-BQpFw_X2XZZR/view?usp=sharing\n",
        "\n",
        "#download Kaggle Files from Google Drive to store it in Colab Session\n",
        "downloaded = drive.CreateFile({'id':\"17cD6hFS_AfKxtfjKxfeNUkC44jfSYkGj\"})   \n",
        "downloaded.GetContentFile('train.csv') \n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1iEpwQ3B4d4gIUl3ynq_p74U1QKZTURem\"})   \n",
        "downloaded.GetContentFile('train_result.csv')    \n",
        "\n",
        "downloaded = drive.CreateFile({'id':\"1QbhqakgLpBXWr5sMFSe-BQpFw_X2XZZR\"})   \n",
        "downloaded.GetContentFile('test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gc8y7p62l006",
        "outputId": "c75075ca-234b-4f55-e2a0-ebaf0129186d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount your Google Drive to Collaboratory\n",
        "# If the google.colab library written in top then it won't run the code\n",
        "from google.colab import drive \n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RF2vr98u1J8o",
        "outputId": "df7c33bd-0f57-4d43-b646-5b9b48e17722"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(1040323, 2)\n",
            "(1040323,)\n",
            "(560175, 2)\n"
          ]
        }
      ],
      "source": [
        "# Load train data into dataframe and numpy array\n",
        "df_train = pd.read_csv('train.csv')\n",
        "train = df_train.to_numpy()\n",
        "train_inputs = train[:]\n",
        "print(train_inputs.shape)\n",
        "\n",
        "# Load train labels into dataframe and numpy array\n",
        "df_train_result = pd.read_csv('train_result.csv')\n",
        "train_results = df_train_result.to_numpy()\n",
        "train_labels = train_results[:,1]\n",
        "print(train_labels.shape)\n",
        "\n",
        "# Load test data into dataframes and numpy array\n",
        "df_test = pd.read_csv('test.csv')\n",
        "test = df_test.to_numpy()\n",
        "test_inputs = test[:]\n",
        "print(test_inputs.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "d2llx4Hp-jQL"
      },
      "outputs": [],
      "source": [
        "# Changing the character classes to numerical classes via values to 0,1,2\n",
        "train_labels[train_labels=='negative']=0\n",
        "train_labels[train_labels=='neutral']=1\n",
        "train_labels[train_labels=='positive']=2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LEz9i6JI16sP",
        "outputId": "00a9af6d-61ee-4659-f0b9-5cdd74e75e24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0 1 2]\n",
            "[59667 'Is lonely '] 0\n",
            "[353467\n",
            " 'a clockwork orange, breakfast, pizza party with the pixel farm dudes, possibly tutoring, bike adventure with my amazing boyfriend ']\n",
            "[0.49965539548774757, 8.074415349848076e-05, 0.5002638603587539]\n"
          ]
        }
      ],
      "source": [
        "# Smoke Tests\n",
        "list_classes = np.unique(train_labels)\n",
        "print(list_classes)\n",
        "\n",
        "n = np.random.randint(0, len(train_inputs))\n",
        "print(train_inputs[n], train_labels[n])\n",
        "\n",
        "n = np.random.randint(0, len(test_inputs))\n",
        "print(test_inputs[n])\n",
        "\n",
        "# Data distribution between the three classes\n",
        "dist_train_labels = [np.sum(train_labels==0)/len(train_labels),\n",
        "  np.sum(train_labels==1)/len(train_labels),\n",
        "  np.sum(train_labels==2)/len(train_labels)]\n",
        "\n",
        "print(dist_train_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkdjZxCE5rbP"
      },
      "source": [
        "##Generate submission file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "wvp8zXNM5u9C"
      },
      "outputs": [],
      "source": [
        "def prepareKaggleFile(test_inputs, test_predictions, file='tests_label.csv'):\n",
        "    \"\"\"\n",
        "    Make submission file from numpy data\n",
        "    The value of class should be 0, 1 or 2 with 0 being negative, 1 being \n",
        "    neutral and 2 being positive class.\n",
        "    \"\"\"\n",
        "    output_data_for_kaggle = np.zeros((len(test_inputs),2))\n",
        "    for i in range(len(test_inputs)):\n",
        "      output_data_for_kaggle[i,0] = i\n",
        "      output_data_for_kaggle[i,1] = test_predictions[i]\n",
        "\n",
        "    output_data_for_kaggle = output_data_for_kaggle.astype(int)\n",
        "\n",
        "    print(output_data_for_kaggle)\n",
        "\n",
        "    df = pd.DataFrame(data=output_data_for_kaggle,columns=['id','target'])\n",
        "    df.to_csv(file,index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P77zweatljHf"
      },
      "source": [
        "##LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TuU-tw12LPa2"
      },
      "source": [
        "###Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fkr4qD6VbkB3"
      },
      "outputs": [],
      "source": [
        "def remove_class_imbalance(df_train_result):\n",
        "    \"\"\"\n",
        "    Eradicate data Imbalance\n",
        "    Ratio of 84 neutral samples to 5M positive and negative samples was discovered\n",
        "    Changing the labels from neutral to positive to eradicate the data imbalance problem\n",
        "    \"\"\"\n",
        "    df_train_result[df_train_result[\"target\"]=='negative']=0\n",
        "    df_train_result[df_train_result[\"target\"]=='neutral']=2\n",
        "    df_train_result[df_train_result[\"target\"]=='positive']=2\n",
        "    return df_train_result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "i0ayt2sbIxzn"
      },
      "outputs": [],
      "source": [
        "def change_categorical(y_train):\n",
        "    \"\"\"\n",
        "    This function will convert the y_test into categorical.\n",
        "    \"\"\"\n",
        "    y_train = y_train['target']\n",
        "    print(type(y_train))\n",
        "    # Convert y_train into categorical column\n",
        "    # This will help column to save from treating as rank 0>2\n",
        "    Y_train = np.array(pd.get_dummies(y_train))\n",
        "    return Y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MgAwaJ7A7-UN"
      },
      "outputs": [],
      "source": [
        "def remove_punct(text):\n",
        "    \"\"\"\n",
        "    Remove punctuation and numbers.\n",
        "    \"\"\"\n",
        "    text  = \"\".join([char for char in text if char not in string.punctuation])\n",
        "    text = re.sub('[0-9]+', '', text)\n",
        "    return text\n",
        "\n",
        "def stemming(text):\n",
        "    \"\"\"\n",
        "    Snowball Stemmer is also known as the Porter2 stemming algorithm because it \n",
        "    is a better version of the Porter Stemmer. It is more aggressive than \n",
        "    Porter Stemmer.\n",
        "    \"\"\"\n",
        "    text = [snowBallStemmer.stem(word) for word in text.split()]\n",
        "    return ' '.join(text)\n",
        "\n",
        "def preprocessing_cleanning(df_data):\n",
        "      \"\"\"\n",
        "      This function is to preprocess the sentences based on numerous factors.\n",
        "      \"\"\"\n",
        "      # Converting all the upper case to lower case to avoid the distinction between them\n",
        "      df_data['text'] = df_data.text.str.lower()\n",
        "\n",
        "      # Putting the regex for removing the https and www URLs\n",
        "      df_data.text = df_data.text.apply(lambda x: re.sub(r'https?:\\/\\/\\S+', '', x))\n",
        "      df_data.text = df_data.text.apply(lambda x: re.sub(r\"www\\.[a-z]?\\.?(com)+|[a-z]+\\.(com)\", '', x))\n",
        "\n",
        "      # Remove the video and links\n",
        "      df_data.text = df_data.text.apply(lambda x: re.sub(r'{link}', '', x))\n",
        "      df_data.text = df_data.text.apply(lambda x: re.sub(r\"\\[video\\]\", '', x))\n",
        "\n",
        "      # Remove html reference characters\n",
        "      df_data.text = df_data.text.apply(lambda x: re.sub(r'&[a-z]+;', '', x))\n",
        "\n",
        "      # Remove usernames\n",
        "      df_data.text = df_data.text.apply(lambda x: re.sub(r'@[^\\s]+', '', x))\n",
        "\n",
        "      # Removing numbers\n",
        "      df_data.text = df_data.text.apply(lambda x: re.sub(r'\\d+', '', x))\n",
        "\n",
        "      # Removing hashmarks, non-letter characters\n",
        "      df_data.text = df_data.text.apply(lambda x: re.sub(r\"[^a-z\\s\\(\\-:\\)\\\\\\/\\];='#]\", '', x))\n",
        "\n",
        "      # Remove punctuation\n",
        "      df_data['text'] = df_data['text'].apply(lambda x: remove_punct(x))\n",
        "\n",
        "      # Remove stop words\n",
        "      stopword_list = stopwords.words('english')\n",
        "      df_data['text'] = df_data['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopword_list)]))\n",
        "\n",
        "      # stemming\n",
        "      df_data['text'] = df_data['text'].apply(lambda x: stemming(x))\n",
        "\n",
        "      return df_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YErJErdSJrHE"
      },
      "source": [
        "###Prepare dataset, split, tokeniser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "SA9KZaWv0ibU"
      },
      "outputs": [],
      "source": [
        "def tokenizer_df(df_train, df_test):\n",
        "    \"\"\"\n",
        "    This tokenizer allows to vectorize a text corpus, by turning each text into \n",
        "    either a sequence of integers (each integer being the index of a token in a \n",
        "    dictionary) or into a vector where the coefficient for each token could be \n",
        "    binary, based on word count, based on tf-idf.\n",
        "    By default, lowercase is true for tokenizer.\n",
        "    \"\"\"\n",
        "    max_features = 10000\n",
        "    tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "    df_all_text = df_train['text'].append(df_test['text'])\n",
        "    tokenizer.fit_on_texts(df_all_text.values)\n",
        "    # Train: 1040323 rows with 116 features, i.e., (1040323, 116)\n",
        "    X_train = tokenizer.texts_to_sequences(df_train['text'].values)\n",
        "    X_train = pad_sequences(X_train)\n",
        "    # Test: 560175 rows with 40 features, i.e., (560175, 40)\n",
        "    X_test = tokenizer.texts_to_sequences(df_test['text'].values)\n",
        "    X_test = pad_sequences(X_test)\n",
        "    # To equalize the features of train and test set (560175, 116)\n",
        "    X_test = np.lib.pad(X_test, ((0,0),(X_train.shape[1] - X_test.shape[1],0)), \n",
        "                        'constant', constant_values=(0))\n",
        "    return X_train, X_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IswKCa7_JcvW"
      },
      "source": [
        "###Create model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Hifo34uy0i_3"
      },
      "outputs": [],
      "source": [
        "def model(X_train):\n",
        "    \"\"\"\n",
        "    Model creation function: LSTM network\n",
        "    Max features > 10000 did not contribute in increasing the features of \n",
        "    X_train and X_test. This is the same parameter used in tokenizer.\n",
        "    The embed_dim parameter is the length of the vector.\n",
        "    \"\"\"\n",
        "    max_features = 10000\n",
        "    embed_dim = 256\n",
        "    model = Sequential()\n",
        "    \"\"\"\n",
        "    An embedding layer allows us to convert each word into a fixed-length vector \n",
        "    and that is a better way to represent those words along with reduced dimensions. \n",
        "    For each word and input_length is the maximum length of a sequence.\n",
        "    \"\"\"\n",
        "    model.add(Embedding(max_features, embed_dim,input_length = X_train.shape[1]))\n",
        "    \"\"\"\n",
        "    The main purpose of the spatial dropout layer is to avoid overfitting and that \n",
        "    is done by probabilistically removing the inputs of this layer (or the \n",
        "    output of the embedding layer in the network we’re building).\n",
        "    In the end, the nodes of the network are more robust to the future inputs and \n",
        "    tend to not overfit.\n",
        "    \"\"\"\n",
        "    model.add(SpatialDropout1D(0.2))\n",
        "    \"\"\"\n",
        "    The first parameter lstm_out that we’ve defined as 512 it’s the dimensionality \n",
        "    of the output space, and we can choose an even larger number trying to improve \n",
        "    our model, but that can lead to many problems like overfitting and a long \n",
        "    training time.\n",
        "    The dropout parameter is applied to the inputs and/or outputs of our model \n",
        "    (the linear transformations), while the recurrent dropout is applied to the \n",
        "    recurrent state, or cell state, of the model. Recurrent dropout affects the \n",
        "    “memory” of the network. The small the dataset is, the small the value of \n",
        "    recurrent_dropout.\n",
        "    \"\"\"\n",
        "    lstm_out = 512\n",
        "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.1))\n",
        "    model.add(Dense(2,activation='sigmoid'))\n",
        "    \"\"\"\n",
        "    An implementation of the Adam algorithm, which is a robust extended version of \n",
        "    the stochastic gradient descent.\n",
        "    \"\"\"\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fsf4ASTTI3Ni"
      },
      "source": [
        "###Training, save model, prediction and output file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XpuChw2TJ00u",
        "outputId": "b6235a44-04f5-4dee-d915-728d275ca0da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.series.Series'>\n"
          ]
        }
      ],
      "source": [
        "# Remove imbalance\n",
        "df_train_result = remove_class_imbalance(df_train_result)\n",
        "# Change the train_labels into numerical category\n",
        "Y_train = change_categorical(df_train_result)\n",
        "# Tokenize the train and test datasets\n",
        "X_train, X_test = tokenizer_df(df_train, df_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VloIQcx54YI",
        "outputId": "304499f0-7ba8-4d69-9347-d25ae825959b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 116, 256)          2560000   \n",
            "                                                                 \n",
            " spatial_dropout1d (SpatialD  (None, 116, 256)         0         \n",
            " ropout1D)                                                       \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 512)               1574912   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 2)                 1026      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4,135,938\n",
            "Trainable params: 4,135,938\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "# Model Summary\n",
        "model = model(X_train)\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "AmZoTbNrEzoi"
      },
      "outputs": [],
      "source": [
        "# Reproducible results from Model\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zHAwVjev54ae",
        "outputId": "a54624f3-da06-46e7-d128-4dcb058effd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16256/16256 - 7431s - loss: 0.4169 - accuracy: 0.8078 - 7431s/epoch - 457ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd4ae787110>"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "The batch size is the number of samples to run through the network before a \n",
        "weight update is performed (an epoch), we’ll keep it low as it requires less \n",
        "memory.\n",
        "\"\"\"\n",
        "batch_size = 64\n",
        "model.fit(X_train, Y_train, epochs = 1, batch_size=batch_size, verbose = 2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ykZDFrpgnBaA"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "filename = 'gdrive/MyDrive/models/neural_network_lstm.h5'\n",
        "model.save(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kLEdoT3M54ii",
        "outputId": "1a2e8dce-ad34-49bc-dcce-7ec7b1529f5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17506/17506 [==============================] - 674s 39ms/step\n"
          ]
        }
      ],
      "source": [
        "# Predict the test set\n",
        "y_predicted = model.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgptKD2zixu8",
        "outputId": "02d55e1d-ec03-4939-c6d8-b1c34bca28f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[     0      0]\n",
            " [     1      2]\n",
            " [     2      0]\n",
            " ...\n",
            " [560172      0]\n",
            " [560173      0]\n",
            " [560174      2]]\n"
          ]
        }
      ],
      "source": [
        "# The output class is the one which will have highest probability of neuron\n",
        "\"\"\"\n",
        "Source:\n",
        "# https://datascience.stackexchange.com/questions/79761/class-label-prediction-in-keras-sequential-model-showing-different-results-in-co\n",
        "# https://stackoverflow.com/questions/68776790/model-predict-classes-is-deprecated-what-to-use-instead\n",
        "# evidently the decided label should be the output neuron with the highest probability.\n",
        "# https://datascience.stackexchange.com/questions/36238/what-does-the-output-of-model-predict-function-from-keras-mean\n",
        "\"\"\"\n",
        "pred = np.argmax(y_predicted, axis=1)*2\n",
        "prepareKaggleFile(test_inputs, pred, file='gdrive/MyDrive/test_label_nn_lstm.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75UbBMLBayyk",
        "outputId": "6ed0eea3-debd-44cb-e4fc-ab55d9ab02bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "Load the model - We applied model each time by saving the model for 1 epoch and \n",
        "load the model for testing epoch afterwards. \n",
        "Epoch 1: Train set-80.78, Test set: 0.82497\n",
        "Epoch 2: Train set-83.12, Test set: 0.82816\n",
        "Epoch 3: Train set-84.24, Test set: 0.83065\n",
        "Epoch 4: Train set-85.13, Test set: 0.83051 (Stopped Here)\n",
        "\"\"\"\n",
        "filename = 'gdrive/MyDrive/models/neural_network_lstm.h5'\n",
        "model = keras.models.load_model(filename)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMIYKs3L8NqQ",
        "outputId": "04fd92c2-f6d8-4149-e485-29797eef7c15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16256/16256 - 7456s - loss: 0.3751 - accuracy: 0.8312 - 7456s/epoch - 459ms/step\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fd4a9b39c50>"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "The batch size is the number of samples to run through the network before a \n",
        "weight update is performed (an epoch), we’ll keep it low as it requires less \n",
        "memory.\n",
        "\"\"\"\n",
        "batch_size = 64\n",
        "model.fit(X_train, Y_train, epochs = 1, batch_size=batch_size, verbose = 2, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "4KsdgFxk8Xwm"
      },
      "outputs": [],
      "source": [
        "# Save the model\n",
        "filename = 'gdrive/MyDrive/models/neural_network_lstm.h5'\n",
        "model.save(filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skX5WE2ebFk_"
      },
      "source": [
        "Finished"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "JcyTy6CXyQ6Y",
        "MkdjZxCE5rbP",
        "TuU-tw12LPa2",
        "YErJErdSJrHE",
        "IswKCa7_JcvW"
      ],
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "05d8f42d34e97b63e1918e352e5e1ee86173089fb3b8c3e567ea1c06d83cd6aa"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
